{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from diffusers import ControlNetModel, DDIMScheduler\n",
    "from PIL import Image\n",
    "from pipelines.controlnet_inpainting_pipeline import StableDiffusionControlNetInpaintPipeline\n",
    "from settings.prompt_enhancer import PromptEnhancer\n",
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
    "\n",
    "from .ade import ade_palette, ADE_CLASSES, NEG_CLASSES, INIT_CLASSES\n",
    "from .segformer_model import SegFormer\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounded_rectangle(src, top_left, bottom_right, radius=1, color=255, thickness=1, line_type=cv2.LINE_AA):\n",
    "    \"\"\" Function to create a rouned rectangle\n",
    "        Stackoverflow: https://stackoverflow.com/a/60210706\n",
    "    \"\"\"\n",
    "    #  corners:\n",
    "    #  p1 - p2\n",
    "    #  |     |\n",
    "    #  p4 - p3\n",
    "\n",
    "    p1 = top_left\n",
    "    p2 = (bottom_right[1], top_left[1])\n",
    "    p3 = (bottom_right[1], bottom_right[0])\n",
    "    p4 = (top_left[0], bottom_right[0])\n",
    "\n",
    "    height = abs(bottom_right[0] - top_left[1])\n",
    "\n",
    "    if radius > 1:\n",
    "        radius = 1\n",
    "\n",
    "    corner_radius = int(radius * (height/2))\n",
    "\n",
    "    if thickness < 0:\n",
    "\n",
    "        #big rect\n",
    "        top_left_main_rect = (int(p1[0] + corner_radius), int(p1[1]))\n",
    "        bottom_right_main_rect = (int(p3[0] - corner_radius), int(p3[1]))\n",
    "\n",
    "        top_left_rect_left = (p1[0], p1[1] + corner_radius)\n",
    "        bottom_right_rect_left = (p4[0] + corner_radius, p4[1] - corner_radius)\n",
    "\n",
    "        top_left_rect_right = (p2[0] - corner_radius, p2[1] + corner_radius)\n",
    "        bottom_right_rect_right = (p3[0], p3[1] - corner_radius)\n",
    "\n",
    "        all_rects = [\n",
    "        [top_left_main_rect, bottom_right_main_rect], \n",
    "        [top_left_rect_left, bottom_right_rect_left], \n",
    "        [top_left_rect_right, bottom_right_rect_right]]\n",
    "\n",
    "        [cv2.rectangle(src, rect[0], rect[1], color, thickness) for rect in all_rects]\n",
    "\n",
    "    # draw straight lines\n",
    "    cv2.line(src, (p1[0] + corner_radius, p1[1]), (p2[0] - corner_radius, p2[1]), color, abs(thickness), line_type)\n",
    "    cv2.line(src, (p2[0], p2[1] + corner_radius), (p3[0], p3[1] - corner_radius), color, abs(thickness), line_type)\n",
    "    cv2.line(src, (p3[0] - corner_radius, p4[1]), (p4[0] + corner_radius, p3[1]), color, abs(thickness), line_type)\n",
    "    cv2.line(src, (p4[0], p4[1] - corner_radius), (p1[0], p1[1] + corner_radius), color, abs(thickness), line_type)\n",
    "\n",
    "    # draw arcs\n",
    "    cv2.ellipse(src, (p1[0] + corner_radius, p1[1] + corner_radius), (corner_radius, corner_radius), 180.0, 0, 90, color ,thickness, line_type)\n",
    "    cv2.ellipse(src, (p2[0] - corner_radius, p2[1] + corner_radius), (corner_radius, corner_radius), 270.0, 0, 90, color , thickness, line_type)\n",
    "    cv2.ellipse(src, (p3[0] - corner_radius, p3[1] - corner_radius), (corner_radius, corner_radius), 0.0, 0, 90,   color , thickness, line_type)\n",
    "    cv2.ellipse(src, (p4[0] + corner_radius, p4[1] - corner_radius), (corner_radius, corner_radius), 90.0, 0, 90,  color , thickness, line_type)\n",
    "\n",
    "    return src\n",
    "\n",
    "def overlay(image, mask, color, alpha, resize=None):\n",
    "    \"\"\"Combines image and its segmentation mask into a single image.\n",
    "    https://www.kaggle.com/code/purplejester/showing-samples-with-segmentation-mask-overlay\n",
    "\n",
    "    Params:\n",
    "        image: Training image. np.ndarray,\n",
    "        mask: Segmentation mask. np.ndarray,\n",
    "        color: Color for segmentation mask rendering.  tuple[int, int, int] = (255, 0, 0)\n",
    "        alpha: Segmentation mask's transparency. float = 0.5,\n",
    "        resize: If provided, both image and its mask are resized before blending them together.\n",
    "        tuple[int, int] = (1024, 1024))\n",
    "\n",
    "    Returns:\n",
    "        image_combined: The combined image. np.ndarray\n",
    "\n",
    "    \"\"\"\n",
    "    color = color[::-1]\n",
    "    colored_mask = np.expand_dims(mask, 0).repeat(3, axis=0)\n",
    "    colored_mask = np.moveaxis(colored_mask, 0, -1)\n",
    "    masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
    "    image_overlay = masked.filled()\n",
    "\n",
    "    if resize is not None:\n",
    "        image = cv2.resize(image.transpose(1, 2, 0), resize)\n",
    "        image_overlay = cv2.resize(image_overlay.transpose(1, 2, 0), resize)\n",
    "\n",
    "    image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
    "\n",
    "    return image_combined\n",
    "\n",
    "def rescale_image(image, size=(512, 512), pad=True):\n",
    "    W, H = size\n",
    "    if pad:\n",
    "        image.thumbnail((512, 512))\n",
    "        rW, rH = image.size\n",
    "        temp_input = Image.new(image.mode, size)\n",
    "        temp_input.paste(image, (W//2 - rW//2,\n",
    "                                        H//2 - rH//2))\n",
    "        size = (rW, rH)\n",
    "    else:\n",
    "        temp_input = image.resize(size)\n",
    "\n",
    "    return temp_input, size\n",
    "\n",
    "def unpad_image(padded_img, size):\n",
    "    padded_img = padded_img.copy()\n",
    "    \n",
    "    W, H = padded_img.size\n",
    "    rW, rH = size\n",
    "    cx, cy = W//2, H//2\n",
    "    icx, icy = rW//2, rH//2\n",
    "    x1, y1 = cx - icx, cy - icy\n",
    "    x2, y2 = cx + icx, cy + icy\n",
    "\n",
    "    return padded_img.crop((x1, y1, x2, y2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlNetInpaint:\n",
    "    def __init__(\n",
    "            self,\n",
    "            cn_model='lllyasviel/sd-controlnet-seg',\n",
    "            # cn_model='BertChristiaens/controlnet-seg-room',\n",
    "            sd_model='runwayml/stable-diffusion-inpainting',\n",
    "            use_cuda=True\n",
    "        ) -> None:\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        # Device to use\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        # Load all models\n",
    "        self.load_models(sd_model, cn_model)\n",
    "\n",
    "    def load_models(self, sd_model, cn_model):\n",
    "        # Load controlnet\n",
    "        self.cn_model = ControlNetModel.from_pretrained(\n",
    "                cn_model,\n",
    "                torch_dtype=torch.float16\n",
    "        )\n",
    "        # Load stable diffusion\n",
    "        self.pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "                sd_model,\n",
    "                controlnet=self.cn_model,\n",
    "                torch_dtype=torch.float16\n",
    "        )\n",
    "        self.pipe.scheduler = DDIMScheduler.from_config(self.pipe.scheduler.config)\n",
    "\n",
    "        # Memory optimisation\n",
    "        if self.use_cuda:\n",
    "            self.pipe.enable_xformers_memory_efficient_attention()\n",
    "        self.pipe.to(self.device)\n",
    "\n",
    "        # Load the prompt enhancer\n",
    "        # self.prompt_enhancer = PromptEnhancer()\n",
    "\n",
    "        # Load segmentation models\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\n",
    "            'openmmlab/upernet-convnext-small'\n",
    "        )\n",
    "        self.image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\n",
    "            'openmmlab/upernet-convnext-small'\n",
    "        )\n",
    "\n",
    "    def get_cn_seg_control(self, image, return_mask=False,\n",
    "                           dilation_kernel=(5, 5),\n",
    "                           iterations=10,\n",
    "                           mask_option='erode',\n",
    "                           classes=INIT_CLASSES,\n",
    "                           neg_classes=NEG_CLASSES):\n",
    "        # Pre-process images\n",
    "        pixel_values = self.image_processor(\n",
    "                np.array(image),\n",
    "                return_tensors='pt'\n",
    "        ).pixel_values\n",
    "\n",
    "        # Run image segmentation\n",
    "        with torch.no_grad():\n",
    "            seg = self.image_segmentor(pixel_values)\n",
    "\n",
    "        # Refine segmentation\n",
    "        seg = self.image_processor.post_process_semantic_segmentation(\n",
    "            seg, target_sizes=[image.size[::-1]]\n",
    "        )[0]\n",
    "        class_labels = []\n",
    "        class_idxs = []\n",
    "        # If True it will generate a mask for inpainting otherwise segmantation image for controll net will be generated\n",
    "        if return_mask:            \n",
    "            if not len(classes):\n",
    "                classes = ADE_CLASSES\n",
    "                \n",
    "            mask = seg.cpu().numpy().copy()\n",
    "            # Extracting detected labels and ids\n",
    "            for i, class_label in enumerate(classes):\n",
    "                if class_label in neg_classes:\n",
    "                    continue\n",
    "                class_idx = ADE_CLASSES.index(class_label)\n",
    "                if np.any(mask==class_idx):\n",
    "                    class_labels.append(class_label)\n",
    "                    class_idxs.append(class_idx)\n",
    "            print(class_labels)\n",
    "\n",
    "            # creating a mask of detected objects\n",
    "            for idx in class_idxs:\n",
    "                mask[mask==idx] = 255\n",
    "            mask[mask!=255] = 0\n",
    "\n",
    "            # Expanding or shinking mask based on `mask_option`\n",
    "            mask = mask.astype(np.float32)\n",
    "            kernel = np.ones(dilation_kernel, np.uint8)\n",
    "            if mask_option == 'erode':\n",
    "                mask = cv2.erode(mask, kernel, iterations=iterations)\n",
    "            else:\n",
    "                mask = cv2.dilate(mask, kernel, iterations=iterations)\n",
    "\n",
    "            image = Image.fromarray(mask.astype(np.uint8))\n",
    "\n",
    "        else:  \n",
    "            # Color code using ADE palette\n",
    "            color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
    "            for label, color in enumerate(np.array(ade_palette())):\n",
    "                color_seg[seg==label] = color\n",
    "            color_seg = color_seg.astype(np.uint8)\n",
    "            image = Image.fromarray(color_seg)\n",
    "    \n",
    "        return image\n",
    "\n",
    "    def get_prompts(\n",
    "            self,\n",
    "            image,\n",
    "            room_type=None,\n",
    "            architecture_style=None,\n",
    "            override_prompt=None\n",
    "        ):\n",
    "        # Generate prompts\n",
    "        prompt, add_prompt, negative_prompt = vs.create_prompts(\n",
    "                room_type=room_type,\n",
    "                architecture_style=architecture_style\n",
    "        )\n",
    "        prompt = prompt + ', ' + add_prompt\n",
    "\n",
    "        # Override prompt if user has specified\n",
    "        if override_prompt:\n",
    "            prompt = override_prompt\n",
    "\n",
    "        # Enhance prompt\n",
    "        # prompt = random.choice(self.prompt_enhancer(prompt))\n",
    "\n",
    "        return prompt, negative_prompt\n",
    "\n",
    "    def run_model(\n",
    "            self,\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            image,\n",
    "            mask,\n",
    "            controlnet_image,\n",
    "            num_inference_step,\n",
    "            guidance_scale,\n",
    "            control_strength,\n",
    "            generator\n",
    "        ):\n",
    "        # Text prompt\n",
    "        input_ids = self.pipe.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors='pt'\n",
    "        ).input_ids.to(self.device)\n",
    "        negative_ids = self.pipe.tokenizer(\n",
    "                negative_prompt,\n",
    "                truncation=False,\n",
    "                padding='max_length',\n",
    "                max_length=input_ids.shape[-1],\n",
    "                return_tensors='pt'\n",
    "        ).input_ids.to(self.device)\n",
    "\n",
    "        # Encode prompts in chunks because of max_length limit\n",
    "        concat_embeds, neg_embeds = [], []\n",
    "        max_length = self.pipe.tokenizer.model_max_length\n",
    "        for i in range(0, input_ids.shape[-1], max_length):\n",
    "            concat_embeds.append(\n",
    "                    self.pipe.text_encoder(input_ids[:,i:i+max_length])[0]\n",
    "            )\n",
    "            neg_embeds.append(\n",
    "                    self.pipe.text_encoder(negative_ids[:,i:i+max_length])[0]\n",
    "            )\n",
    "\n",
    "        # Concat chunks\n",
    "        prompt_embeds = torch.cat(concat_embeds, dim=1)\n",
    "        negative_prompt_embeds = torch.cat(neg_embeds, dim=1)\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds[:, :prompt_embeds.shape[1], :] \n",
    "        # Run through pipe\n",
    "        images = self.pipe(\n",
    "                image=image,\n",
    "                mask_image=mask,\n",
    "                control_image=controlnet_image,\n",
    "                num_images_per_prompt=1,\n",
    "                num_inference_steps=num_inference_step,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                controlnet_conditioning_scale=control_strength,\n",
    "                prompt_embeds=prompt_embeds,\n",
    "                negative_prompt_embeds=negative_prompt_embeds\n",
    "        ).images\n",
    "        return images\n",
    "\n",
    "    def generate_mask(self, image, mask_dilation,\n",
    "                       mask_option, use_rounded=False):\n",
    "        \"\"\"Function to only run and display SegFormer for mask debugging.\n",
    "            integrated with `Generate Mask` button in gradio app.\n",
    "        \"\"\"\n",
    "        if isinstance(image, dict):\n",
    "            image = image['image'].convert('RGB')\n",
    "        \n",
    "        mask = self.get_mask(image, mask_dilation, mask_option=mask_option,\n",
    "                             use_rounded=use_rounded)\n",
    "        mask = Image.fromarray(overlay(np.array(image),\n",
    "                                       np.array(mask),\n",
    "                                       (255, 0, 0), 0.5))\n",
    "        return [mask]\n",
    "\n",
    "    def get_mask(self, image, mask_dilation,\n",
    "                  mask_option, use_rounded=False,\n",
    "                  classes=INIT_CLASSES,\n",
    "                  padding=10, \n",
    "                  neg_classes=NEG_CLASSES):\n",
    "        W, H = image.size\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "        smask = self.get_cn_seg_control(image, return_mask=True,\n",
    "                                iterations=mask_dilation,\n",
    "                                mask_option=mask_option,\n",
    "                                classes=classes,\n",
    "                                neg_classes=neg_classes)\n",
    "\n",
    "        if use_rounded:\n",
    "            smask = np.array(smask)\n",
    "            mask = rounded_rectangle(np.zeros_like(smask), (0 + padding, 0 + padding),\n",
    "                                     (H - padding, W - padding), radius=0.5, color=(255, 255, 255), thickness=-1)\n",
    "            mask = mask//255\n",
    "            smask = smask//255\n",
    "            smask = mask*smask\n",
    "            smask *= 255    \n",
    "            smask = Image.fromarray(smask)\n",
    "        smask = cv2.resize(\n",
    "                np.array(smask), (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "        smask = Image.fromarray(smask)\n",
    "\n",
    "        return smask\n",
    "\n",
    "    def run_single_iteratiation(self,\n",
    "                                input_image,\n",
    "                                mask_image,\n",
    "                                prompt,\n",
    "                                negative_prompt,\n",
    "                                control_strength,\n",
    "                                guidance_scale,\n",
    "                                num_inference_step,\n",
    "                                seed\n",
    "                                ):\n",
    "        rW, rH = input_image.size\n",
    "        # Get control image\n",
    "        control = self.get_cn_seg_control(image=input_image.copy())\n",
    "        control = cv2.resize(\n",
    "                    np.array(control), (rW, rH), interpolation=cv2.INTER_NEAREST)\n",
    "        control = Image.fromarray(control)\n",
    "\n",
    "        # Set seed\n",
    "        if seed == 0:\n",
    "            itr_seed = torch.randint(0, 1000000, (1,))\n",
    "        else:\n",
    "            itr_seed = seed\n",
    "        itr_seed_gen = torch.manual_seed(itr_seed)\n",
    "\n",
    "        print(f'\\tUsing model with seed {itr_seed} '\n",
    "                f'strength {control_strength} '\n",
    "                f'and guidance {guidance_scale} '\n",
    "                f'with prompt: \\n\\t{prompt}')\n",
    "\n",
    "        # Run model\n",
    "        output = self.run_model(\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            input_image,\n",
    "            mask_image,\n",
    "            control,\n",
    "            num_inference_step,\n",
    "            guidance_scale,\n",
    "            control_strength,\n",
    "            itr_seed_gen\n",
    "        )\n",
    "        return output[0]\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            image_dict,\n",
    "            room_type,\n",
    "            architecture_style=None,\n",
    "            negative_prompt=\"\",\n",
    "            num_images_per_prompt=5,\n",
    "            guidance_scale=12,\n",
    "            num_inference_step=20,\n",
    "            strength_min=0.1,\n",
    "            strength_max=0.5,\n",
    "            seed=0,\n",
    "            override_prompt=None,\n",
    "            upscale=False,\n",
    "            mask_dilation=1,\n",
    "            mask_option='dilate',\n",
    "            use_fixed_strength=False,\n",
    "            use_rounded=True,\n",
    "            padding=10\n",
    "        ):\n",
    "        W, H = image_dict['image'].size\n",
    "        org_image = image_dict['image'].copy()\n",
    "        input_image = image_dict['image'].convert('RGB')\n",
    "        input_image = Image.fromarray(utils.resize_image(np.array(input_image), 512))\n",
    "        rW, rH = input_image.size\n",
    "        mask_image = image_dict['mask'].convert('RGB')\n",
    "        mask_image = cv2.resize(\n",
    "                        np.array(mask_image), (rW, rH), interpolation=cv2.INTER_NEAREST)\n",
    "        mask_image = Image.fromarray(mask_image)\n",
    "\n",
    "        if not np.any(np.array(mask_image)):\n",
    "            print('Extracting Mask...')\n",
    "            mask_image = self.get_mask(input_image, mask_dilation,\n",
    "                                        mask_option=mask_option,\n",
    "                                        use_rounded=use_rounded,\n",
    "                                        padding=padding)\n",
    "\n",
    "\n",
    "        strength_factor = (strength_max - strength_min)/num_images_per_prompt\n",
    "        control_strength = strength_max\n",
    "        output_images = []\n",
    "\n",
    "        # Get prompts\n",
    "        prompt, negative_prompt = self.get_prompts(\n",
    "                input_image,\n",
    "                room_type,\n",
    "                architecture_style=architecture_style,\n",
    "                override_prompt=override_prompt\n",
    "        )\n",
    "\n",
    "        min_seed = 1800000000\n",
    "        max_seed = 4200000001\n",
    "        # Why set seed in this range? \n",
    "        # Don't know Magic number maybe\n",
    "\n",
    "        # Set seed\n",
    "        if seed == 0:\n",
    "            seed_value = np.random.randint(min_seed, max_seed)\n",
    "\n",
    "        for i in range(num_images_per_prompt):\n",
    "            # increase control strength iteratively\n",
    "            if not use_fixed_strength:\n",
    "                control_strength = strength_min + (i+1)*strength_factor\n",
    "\n",
    "            seed = torch.randint(max(0, seed_value - 10000), seed_value + 10000, (1,))\n",
    "\n",
    "\n",
    "            output = self.run_single_iteratiation(input_image,\n",
    "                                         mask_image,\n",
    "                                         prompt,\n",
    "                                         negative_prompt,\n",
    "                                         control_strength,\n",
    "                                         guidance_scale,\n",
    "                                         num_inference_step,\n",
    "                                         seed)\n",
    "            output_images.append(output.copy())\n",
    "            input_image = output.copy()\n",
    "\n",
    "        ##################################################################\n",
    "        # Renovate generated image by extracting the objects masks\n",
    "        ##################################################################\n",
    "\n",
    "        seed = np.random.randint(min_seed, max_seed)\n",
    "        \n",
    "        input_image = output_images[-1].copy()        \n",
    "        final_mask = self.get_mask(input_image,\n",
    "                            20,\n",
    "                            mask_option='dilate',\n",
    "                            use_rounded=False,\n",
    "                            padding=padding,\n",
    "                            classes=[],\n",
    "                            neg_classes=NEG_CLASSES+INIT_CLASSES+['light'])\n",
    "\n",
    "        # final_mask.save('stage-2-mask.jpg')\n",
    "        # Combine stage-1 mask and new mask\n",
    "        final_mask = np.array(final_mask.convert('L'))\n",
    "        mask_image = np.array(mask_image.convert('L'))\n",
    "        final_mask = final_mask//255\n",
    "        mask_image = mask_image//255\n",
    "        final_mask = mask_image*final_mask\n",
    "        final_mask *= 255    \n",
    "        final_mask = Image.fromarray(final_mask)\n",
    "        mask_image *= 255\n",
    "        mask_image = Image.fromarray(mask_image)\n",
    "        # final_mask.save('final_mask.jpg')\n",
    "        output = self.run_single_iteratiation(input_image,\n",
    "                                        final_mask,\n",
    "                                        prompt,\n",
    "                                        negative_prompt,\n",
    "                                        0.4,\n",
    "                                        15,\n",
    "                                        num_inference_step,\n",
    "                                        seed)\n",
    "        output_images.append(output.copy())\n",
    "        ##################################################################\n",
    "\n",
    "        # Resize input image to match output image\n",
    "        W, H = output_images[0].size[:2]\n",
    "        \n",
    "        input_image = np.array(rescale_image(org_image.convert('RGB'),\n",
    "                                             size=(W,H), pad=False)[0])\n",
    "        # Resize mask to match input image\n",
    "        mask_image = np.array(rescale_image(mask_image.convert('L'),\n",
    "                                            size=(W, H), pad=False)[0])\n",
    "\n",
    "        # # merge input image and generated image to remove oily effect due to model\n",
    "        output_images[-1] = utils.post_process_image(output_images[-1],\n",
    "                                                     input_image, mask_image)\n",
    "\n",
    "\n",
    "        # Overlay mask on input image\n",
    "        mask = [Image.fromarray(overlay(input_image, mask_image,\n",
    "                                        (255, 0, 0), 0.5))]\n",
    "        output_images = [img for img in output_images]\n",
    "\n",
    "        # Not actually upscalling it's just resizing into original size\n",
    "        if upscale:\n",
    "            output_images = [rescale_image(img, size=org_image.size,\n",
    "                                           pad=False)[0] for img in output_images]\n",
    "            mask = [rescale_image(m, size=org_image.size,\n",
    "                                           pad=False)[0] for m in mask]\n",
    "\n",
    "        return output_images, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
